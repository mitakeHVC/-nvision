# Phase 2 ãƒ†ã‚¹ãƒˆæˆ¦ç•¥ã¨Gitæˆ¦ç•¥

## ğŸ§ª ãƒ†ã‚¹ãƒˆæˆ¦ç•¥ã®è©³ç´°åŒ–

### ãƒ†ã‚¹ãƒˆç¨®é¡ã¨å®Ÿè¡Œã‚¿ã‚¤ãƒŸãƒ³ã‚°

| ãƒ†ã‚¹ãƒˆç¨®é¡ | å®Ÿè¡Œã‚¿ã‚¤ãƒŸãƒ³ã‚° | ã‚«ãƒãƒ¬ãƒƒã‚¸ç›®æ¨™ | å®Ÿè¡Œæ™‚é–“ç›®æ¨™ | è²¬ä»»è€… |
|------------|----------------|----------------|--------------|--------|
| å˜ä½“ãƒ†ã‚¹ãƒˆ | å„ã‚³ãƒŸãƒƒãƒˆæ™‚ | 90%ä»¥ä¸Š | 30ç§’ä»¥å†… | é–‹ç™ºè€… |
| çµ±åˆãƒ†ã‚¹ãƒˆ | PRä½œæˆæ™‚ | 80%ä»¥ä¸Š | 2åˆ†ä»¥å†… | CI/CD |
| E2Eãƒ†ã‚¹ãƒˆ | ãƒãƒ¼ã‚¸å‰ | ä¸»è¦ãƒ•ãƒ­ãƒ¼100% | 5åˆ†ä»¥å†… | CI/CD |
| ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ | ãƒªãƒªãƒ¼ã‚¹å‰ | å…¨API | 10åˆ†ä»¥å†… | QA |
| ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ | ãƒªãƒªãƒ¼ã‚¹å‰ | å…¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ | 15åˆ†ä»¥å†… | QA |

### ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ç›®æ¨™

#### ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆ¥ã‚«ãƒãƒ¬ãƒƒã‚¸
- **ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«**: 95%ä»¥ä¸Š
- **ãƒªãƒã‚¸ãƒˆãƒªå±¤**: 90%ä»¥ä¸Š
- **ã‚µãƒ¼ãƒ“ã‚¹å±¤**: 90%ä»¥ä¸Š
- **APIå±¤**: 85%ä»¥ä¸Š
- **ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢**: 80%ä»¥ä¸Š
- **çµ±åˆãƒ†ã‚¹ãƒˆ**: 80%ä»¥ä¸Š

#### æ©Ÿèƒ½åˆ¥ã‚«ãƒãƒ¬ãƒƒã‚¸
- **CRUDæ“ä½œ**: 100%
- **èªè¨¼ãƒ»èªå¯**: 100%
- **æ¤œç´¢æ©Ÿèƒ½**: 95%ä»¥ä¸Š
- **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: 90%ä»¥ä¸Š
- **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**: ä¸»è¦ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ100%

## ğŸ“‹ ãƒ†ã‚¹ãƒˆå®Ÿè£…è¨ˆç”»

### Step 1: CI/CDåŸºç›¤æ§‹ç¯‰æ™‚ã®ãƒ†ã‚¹ãƒˆ

#### 1.1 GitHub Actionsè¨­å®šãƒ†ã‚¹ãƒˆ
**ãƒ•ã‚¡ã‚¤ãƒ«**: [`/.github/workflows/test-ci.yml`](.github/workflows/test-ci.yml)
```yaml
name: Test CI Pipeline
on:
  workflow_dispatch:  # Manual trigger for testing

jobs:
  test-pipeline:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Test pipeline setup
      run: echo "CI pipeline test successful"
    
    - name: Validate workflow syntax
      run: |
        # Validate YAML syntax
        python -c "import yaml; yaml.safe_load(open('.github/workflows/ci.yml'))"
```

#### 1.2 å“è³ªã‚²ãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ
**ãƒ•ã‚¡ã‚¤ãƒ«**: [`/tests/quality/test_code_quality.py`](tests/quality/test_code_quality.py)
```python
"""Code quality tests."""
import subprocess
import pytest

def test_black_formatting():
    """Test that code is properly formatted with black."""
    result = subprocess.run(
        ["black", "--check", "src", "tests"],
        capture_output=True,
        text=True
    )
    assert result.returncode == 0, f"Code formatting issues: {result.stdout}"

def test_flake8_linting():
    """Test that code passes flake8 linting."""
    result = subprocess.run(
        ["flake8", "src", "tests"],
        capture_output=True,
        text=True
    )
    assert result.returncode == 0, f"Linting issues: {result.stdout}"

def test_mypy_type_checking():
    """Test that code passes mypy type checking."""
    result = subprocess.run(
        ["mypy", "src"],
        capture_output=True,
        text=True
    )
    assert result.returncode == 0, f"Type checking issues: {result.stdout}"
```

### Step 2: FastAPIåŸºç›¤å®Ÿè£…æ™‚ã®ãƒ†ã‚¹ãƒˆ

#### 2.1 ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•ãƒ†ã‚¹ãƒˆ
**ãƒ•ã‚¡ã‚¤ãƒ«**: [`/tests/api/test_application.py`](tests/api/test_application.py)
```python
"""Application startup and configuration tests."""
import pytest
from fastapi.testclient import TestClient
from src.api.main import app

client = TestClient(app)

def test_application_startup():
    """Test that application starts successfully."""
    response = client.get("/")
    assert response.status_code == 200
    assert "nvision API is running" in response.json()["message"]

def test_openapi_schema_generation():
    """Test that OpenAPI schema is generated correctly."""
    response = client.get("/openapi.json")
    assert response.status_code == 200
    schema = response.json()
    assert "openapi" in schema
    assert schema["info"]["title"] == "nvision API"

def test_docs_endpoints():
    """Test that documentation endpoints are accessible."""
    # Swagger UI
    response = client.get("/docs")
    assert response.status_code == 200
    
    # ReDoc
    response = client.get("/redoc")
    assert response.status_code == 200

def test_cors_middleware():
    """Test CORS middleware configuration."""
    response = client.options("/", headers={
        "Origin": "http://localhost:3000",
        "Access-Control-Request-Method": "GET"
    })
    assert response.status_code == 200
    assert "access-control-allow-origin" in response.headers
```

#### 2.2 ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢ãƒ†ã‚¹ãƒˆ
**ãƒ•ã‚¡ã‚¤ãƒ«**: [`/tests/api/test_middleware.py`](tests/api/test_middleware.py)
```python
"""Middleware tests."""
import pytest
import time
from unittest.mock import patch
from fastapi.testclient import TestClient
from src.api.main import app

client = TestClient(app)

def test_logging_middleware():
    """Test logging middleware functionality."""
    with patch('src.api.middleware.logger') as mock_logger:
        response = client.get("/api/v1/health")
        assert response.status_code == 200
        
        # Verify logging calls
        assert mock_logger.info.call_count >= 2  # Request and response logs
        assert "X-Process-Time" in response.headers

def test_error_handling_middleware():
    """Test error handling middleware."""
    # This would test a route that intentionally raises an exception
    # For now, we'll test that the middleware is properly configured
    assert hasattr(app, 'middleware_stack')

def test_process_time_header():
    """Test that process time header is added."""
    response = client.get("/api/v1/health")
    assert "X-Process-Time" in response.headers
    process_time = float(response.headers["X-Process-Time"])
    assert process_time >= 0
```

#### 2.3 è¨­å®šãƒ†ã‚¹ãƒˆ
**ãƒ•ã‚¡ã‚¤ãƒ«**: [`/tests/api/test_config.py`](tests/api/test_config.py)
```python
"""Configuration tests."""
import pytest
from src.api.config import Settings, get_settings

def test_settings_creation():
    """Test settings creation with default values."""
    settings = Settings()
    assert settings.PROJECT_NAME == "nvision"
    assert settings.API_V1_STR == "/api/v1"
    assert settings.ALGORITHM == "HS256"

def test_settings_caching():
    """Test that settings are cached."""
    settings1 = get_settings()
    settings2 = get_settings()
    assert settings1 is settings2  # Same instance due to lru_cache

def test_environment_variable_override():
    """Test that environment variables override defaults."""
    import os
    original_value = os.environ.get("PROJECT_NAME")
    
    try:
        os.environ["PROJECT_NAME"] = "test-nvision"
        # Clear the cache
        get_settings.cache_clear()
        settings = get_settings()
        assert settings.PROJECT_NAME == "test-nvision"
    finally:
        if original_value:
            os.environ["PROJECT_NAME"] = original_value
        else:
            os.environ.pop("PROJECT_NAME", None)
        get_settings.cache_clear()
```

### Step 3: ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹å±¤å®Ÿè£…æ™‚ã®ãƒ†ã‚¹ãƒˆ

#### 3.1 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ†ã‚¹ãƒˆ
**ãƒ•ã‚¡ã‚¤ãƒ«**: [`/tests/database/test_neo4j_client.py`](tests/database/test_neo4j_client.py)
```python
"""Neo4j client tests."""
import pytest
from unittest.mock import Mock, patch, MagicMock
from src.database.neo4j_client import Neo4jClient

@pytest.fixture
def mock_driver():
    """Mock Neo4j driver."""
    driver = Mock()
    session = Mock()
    driver.session.return_value.__enter__.return_value = session
    driver.session.return_value.__exit__.return_value = None
    return driver, session

def test_neo4j_client_initialization():
    """Test Neo4j client initialization."""
    with patch('src.database.neo4j_client.GraphDatabase.driver') as mock_driver:
        client = Neo4jClient("bolt://localhost:7687", "neo4j", "password")
        mock_driver.assert_called_once_with(
            "bolt://localhost:7687", 
            auth=("neo4j", "password")
        )

def test_verify_connectivity(mock_driver):
    """Test connectivity verification."""
    driver, session = mock_driver
    
    # Mock successful connectivity test
    result = Mock()
    result.single.return_value = {"test": 1}
    session.run.return_value = result
    
    with patch('src.database.neo4j_client.GraphDatabase.driver', return_value=driver):
        client = Neo4jClient("bolt://localhost:7687", "neo4j", "password")
        assert client.verify_connectivity() == True

def test_execute_query(mock_driver):
    """Test query execution."""
    driver, session = mock_driver
    
    # Mock query result
    record1 = Mock()
    record1.data.return_value = {"name": "John", "age": 30}
    record2 = Mock()
    record2.data.return_value = {"name": "Jane", "age": 25}
    
    result = Mock()
    result.__iter__.return_value = [record1, record2]
    session.run.return_value = result
    
    with patch('src.database.neo4j_client.GraphDatabase.driver', return_value=driver):
        client = Neo4jClient("bolt://localhost:7687", "neo4j", "password")
        results = client.execute_query("MATCH (n) RETURN n")
        
        assert len(results) == 2
        assert results[0]["name"] == "John"
        assert results[1]["name"] == "Jane"

def test_execute_write_query(mock_driver):
    """Test write query execution."""
    driver, session = mock_driver
    
    # Mock transaction
    tx = Mock()
    tx.run.return_value = []
    session.write_transaction.return_value = []
    
    with patch('src.database.neo4j_client.GraphDatabase.driver', return_value=driver):
        client = Neo4jClient("bolt://localhost:7687", "neo4j", "password")
        result = client.execute_write_query("CREATE (n:Person {name: $name})", {"name": "John"})
        
        session.write_transaction.assert_called_once()

def test_client_close(mock_driver):
    """Test client connection closing."""
    driver, session = mock_driver
    
    with patch('src.database.neo4j_client.GraphDatabase.driver', return_value=driver):
        client = Neo4jClient("bolt://localhost:7687", "neo4j", "password")
        client.close()
        driver.close.assert_called_once()
```

#### 3.2 ãƒªãƒã‚¸ãƒˆãƒªãƒ†ã‚¹ãƒˆ
**ãƒ•ã‚¡ã‚¤ãƒ«**: [`/tests/repositories/test_base_repository.py`](tests/repositories/test_base_repository.py)
```python
"""Base repository tests."""
import pytest
from abc import ABC
from unittest.mock import Mock
from src.repositories.base_repository import BaseRepository

class TestRepository(BaseRepository):
    """Test implementation of base repository."""
    
    def create(self, entity):
        return entity
    
    def get_by_id(self, entity_id):
        return {"id": entity_id}
    
    def update(self, entity_id, updates):
        return {"id": entity_id, **updates}
    
    def delete(self, entity_id):
        return True
    
    def list(self, limit=100, offset=0):
        return [{"id": i} for i in range(offset, offset + min(limit, 10))]

def test_base_repository_abstract():
    """Test that BaseRepository is abstract."""
    with pytest.raises(TypeError):
        BaseRepository(Mock())

def test_repository_implementation():
    """Test repository implementation."""
    mock_client = Mock()
    repo = TestRepository(mock_client)
    
    # Test create
    entity = {"name": "test"}
    result = repo.create(entity)
    assert result == entity
    
    # Test get_by_id
    result = repo.get_by_id("123")
    assert result["id"] == "123"
    
    # Test update
    result = repo.update("123", {"name": "updated"})
    assert result["id"] == "123"
    assert result["name"] == "updated"
    
    # Test delete
    result = repo.delete("123")
    assert result == True
    
    # Test list
    result = repo.list(5, 0)
    assert len(result) == 5
    assert result[0]["id"] == 0
```

#### 3.3 çµ±åˆãƒ†ã‚¹ãƒˆ
**ãƒ•ã‚¡ã‚¤ãƒ«**: [`/tests/integration/test_data_access_integration.py`](tests/integration/test_data_access_integration.py)
```python
"""Data access integration tests."""
import pytest
from unittest.mock import Mock, patch
from src.services.data_service import DataService
from src.database.connection_manager import ConnectionManager
from src.data_models.crm_models import Customer

@pytest.fixture
def mock_connection_manager():
    """Mock connection manager."""
    manager = Mock(spec=ConnectionManager)
    manager.neo4j = Mock()
    manager.chroma = Mock()
    return manager

@pytest.fixture
def data_service(mock_connection_manager):
    """Data service fixture."""
    return DataService(mock_connection_manager)

@pytest.fixture
def sample_customer():
    """Sample customer fixture."""
    return Customer(
        customer_id="test-123",
        first_name="Test",
        last_name="User",
        email="test@example.com",
        phone="123-456-7890"
    )

def test_data_service_customer_operations(data_service, sample_customer, mock_connection_manager):
    """Test data service customer operations."""
    # Mock repository responses
    with patch('src.services.data_service.CustomerRepository') as MockCustomerRepo:
        mock_repo = MockCustomerRepo.return_value
        mock_repo.create.return_value = sample_customer
        mock_repo.get_by_id.return_value = sample_customer
        mock_repo.list.return_value = [sample_customer]
        
        # Test create
        result = data_service.create_customer(sample_customer)
        assert result.customer_id == "test-123"
        mock_repo.create.assert_called_once_with(sample_customer)
        
        # Test get
        result = data_service.get_customer("test-123")
        assert result.customer_id == "test-123"
        mock_repo.get_by_id.assert_called_once_with("test-123")
        
        # Test list
        results = data_service.list_customers()
        assert len(results) == 1
        assert results[0].customer_id == "test-123"
        mock_repo.list.assert_called_once()

def test_data_service_error_handling(data_service, mock_connection_manager):
    """Test data service error handling."""
    with patch('src.services.data_service.CustomerRepository') as MockCustomerRepo:
        mock_repo = MockCustomerRepo.return_value
        mock_repo.get_by_id.side_effect = Exception("Database error")
        
        with pytest.raises(Exception) as exc_info:
            data_service.get_customer("test-123")
        
        assert "Database error" in str(exc_info.value)
```

## ğŸ”„ Gitæˆ¦ç•¥ã®è©³ç´°åŒ–

### ãƒ–ãƒ©ãƒ³ãƒæˆ¦ç•¥

#### ãƒ–ãƒ©ãƒ³ãƒå‘½åè¦å‰‡
```
main                    # æœ¬ç•ªç’°å¢ƒãƒ–ãƒ©ãƒ³ãƒ
develop                 # é–‹ç™ºçµ±åˆãƒ–ãƒ©ãƒ³ãƒ
feature/[step-name]     # æ©Ÿèƒ½é–‹ç™ºãƒ–ãƒ©ãƒ³ãƒ
hotfix/[issue-name]     # ç·Šæ€¥ä¿®æ­£ãƒ–ãƒ©ãƒ³ãƒ
release/[version]       # ãƒªãƒªãƒ¼ã‚¹ãƒ–ãƒ©ãƒ³ãƒ
```

#### ãƒ–ãƒ©ãƒ³ãƒé‹ç”¨ãƒ•ãƒ­ãƒ¼
```mermaid
gitGraph
    commit id: "Initial"
    branch develop
    checkout develop
    commit id: "Setup"
    
    branch feature/ci-cd-setup
    checkout feature/ci-cd-setup
    commit id: "CI/CD"
    commit id: "Tests"
    
    checkout develop
    merge feature/ci-cd-setup
    
    branch feature/fastapi-foundation
    checkout feature/fastapi-foundation
    commit id: "FastAPI"
    commit id: "Middleware"
    
    checkout develop
    merge feature/fastapi-foundation
    
    branch release/v0.1.0
    checkout release/v0.1.0
    commit id: "Release prep"
    
    checkout main
    merge release/v0.1.0
    tag: "v0.1.0"
    
    checkout develop
    merge release/v0.1.0
```

### ã‚³ãƒŸãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¦å‰‡

#### Conventional Commitså½¢å¼
```
<type>[optional scope]: <description>

[optional body]

[optional footer(s)]
```

#### ã‚¿ã‚¤ãƒ—å®šç¾©
- **feat**: æ–°æ©Ÿèƒ½è¿½åŠ 
- **fix**: ãƒã‚°ä¿®æ­£
- **docs**: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°
- **style**: ã‚³ãƒ¼ãƒ‰ã‚¹ã‚¿ã‚¤ãƒ«å¤‰æ›´ï¼ˆæ©Ÿèƒ½ã«å½±éŸ¿ãªã—ï¼‰
- **refactor**: ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°
- **perf**: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„
- **test**: ãƒ†ã‚¹ãƒˆè¿½åŠ ãƒ»ä¿®æ­£
- **chore**: ãƒ“ãƒ«ãƒ‰ãƒ—ãƒ­ã‚»ã‚¹ãƒ»è£œåŠ©ãƒ„ãƒ¼ãƒ«å¤‰æ›´

#### ã‚³ãƒŸãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä¾‹
```bash
feat(api): implement FastAPI foundation with basic middleware

- Add FastAPI application setup
- Implement CORS and logging middleware
- Add health check endpoints
- Configure OpenAPI documentation

Closes #123
```

### ãƒãƒ¼ã‚¸æˆ¦ç•¥

#### Pull Requestè¦ä»¶
1. **å¿…é ˆãƒã‚§ãƒƒã‚¯**:
   - [ ] CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æˆåŠŸ
   - [ ] ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼æ‰¿èªï¼ˆæœ€ä½1åï¼‰
   - [ ] ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸åŸºæº–é”æˆ
   - [ ] ã‚³ãƒ¼ãƒ‰å“è³ªãƒã‚§ãƒƒã‚¯é€šé

2. **ãƒ¬ãƒ“ãƒ¥ãƒ¼è¦³ç‚¹**:
   - ã‚³ãƒ¼ãƒ‰å“è³ªã¨å¯èª­æ€§
   - ãƒ†ã‚¹ãƒˆã®å¦¥å½“æ€§
   - ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è€ƒæ…®äº‹é …
   - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å½±éŸ¿
   - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°

#### ãƒãƒ¼ã‚¸æ–¹æ³•
- **Squash and merge**: æ©Ÿèƒ½ãƒ–ãƒ©ãƒ³ãƒ â†’ develop
- **Merge commit**: develop â†’ main
- **Fast-forward**: hotfix â†’ main

### Git Hooksè¨­å®š

#### Pre-commit Hook
**ãƒ•ã‚¡ã‚¤ãƒ«**: [`/.git/hooks/pre-commit`](.git/hooks/pre-commit)
```bash
#!/bin/sh
# Pre-commit hook for code quality checks

echo "Running pre-commit checks..."

# Run black formatting check
echo "Checking code formatting..."
black --check src tests
if [ $? -ne 0 ]; then
    echo "âŒ Code formatting check failed. Run 'black src tests' to fix."
    exit 1
fi

# Run flake8 linting
echo "Running linting..."
flake8 src tests
if [ $? -ne 0 ]; then
    echo "âŒ Linting check failed. Fix the issues above."
    exit 1
fi

# Run type checking
echo "Running type checking..."
mypy src
if [ $? -ne 0 ]; then
    echo "âŒ Type checking failed. Fix the issues above."
    exit 1
fi

# Run tests
echo "Running tests..."
pytest tests/ -x
if [ $? -ne 0 ]; then
    echo "âŒ Tests failed. Fix the failing tests."
    exit 1
fi

echo "âœ… All pre-commit checks passed!"
```

#### Commit-msg Hook
**ãƒ•ã‚¡ã‚¤ãƒ«**: [`/.git/hooks/commit-msg`](.git/hooks/commit-msg)
```bash
#!/bin/sh
# Commit message validation hook

commit_regex='^(feat|fix|docs|style|refactor|perf|test|chore)(\(.+\))?: .{1,50}'

if ! grep -qE "$commit_regex" "$1"; then
    echo "âŒ Invalid commit message format!"
    echo "Format: <type>[optional scope]: <description>"
    echo "Example: feat(api): add user authentication"
    exit 1
fi

echo "âœ… Commit message format is valid!"
```

## ğŸ“Š ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã¨ãƒ¬ãƒãƒ¼ãƒˆ

### ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰

#### é–‹ç™ºæ™‚ãƒ†ã‚¹ãƒˆ
```bash
# å˜ä½“ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
pytest tests/unit/ -v

# çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
pytest tests/integration/ -v

# ã‚«ãƒãƒ¬ãƒƒã‚¸ä»˜ããƒ†ã‚¹ãƒˆå®Ÿè¡Œ
pytest --cov=src --cov-report=html --cov-report=term

# ç‰¹å®šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ†ã‚¹ãƒˆ
pytest tests/api/ -v

# å¤±æ•—æ™‚åœæ­¢
pytest -x

# ä¸¦åˆ—å®Ÿè¡Œ
pytest -n auto
```

#### CI/CDãƒ†ã‚¹ãƒˆ
```bash
# å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆCIç’°å¢ƒï¼‰
pytest --cov=src --cov-report=xml --cov-fail-under=85

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
pytest tests/performance/ --benchmark-only

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ
bandit -r src/
safety check
```

### ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

#### ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒãƒ¼ãƒˆ
```bash
# HTMLå½¢å¼ã®ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
pytest --cov=src --cov-report=html
open htmlcov/index.html

# XMLå½¢å¼ï¼ˆCIç”¨ï¼‰
pytest --cov=src --cov-report=xml
```

#### ãƒ†ã‚¹ãƒˆçµæœãƒ¬ãƒãƒ¼ãƒˆ
```bash
# JUnitå½¢å¼ã®ãƒ†ã‚¹ãƒˆçµæœ
pytest --junitxml=test-results.xml

# HTMLå½¢å¼ã®ãƒ†ã‚¹ãƒˆçµæœ
pytest --html=test-report.html --self-contained-html
```

## ğŸš¨ å“è³ªã‚²ãƒ¼ãƒˆã¨è‡ªå‹•åŒ–

### å“è³ªåŸºæº–

| æŒ‡æ¨™ | æœ€å°å€¤ | ç›®æ¨™å€¤ | æ¸¬å®šæ–¹æ³• |
|------|--------|--------|----------|
| ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ | 85% | 90% | pytest-cov |
| ã‚³ãƒ¼ãƒ‰å“è³ªã‚¹ã‚³ã‚¢ | B | A | SonarQube |
| ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚³ã‚¢ | 8/10 | 10/10 | Bandit + Safety |
| ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ | 500ms | 200ms | Locust |
| å¯ç”¨æ€§ | 99% | 99.9% | Uptime monitoring |

### è‡ªå‹•åŒ–ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

#### ç¶™ç¶šçš„ã‚¤ãƒ³ãƒ†ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
1. **ã‚³ãƒ¼ãƒ‰å¤‰æ›´æ¤œçŸ¥** â†’ GitHub webhook
2. **å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ** â†’ GitHub Actions
3. **ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ** â†’ pytest + coverage
4. **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚­ãƒ£ãƒ³** â†’ Bandit + Safety
5. **çµæœé€šçŸ¥** â†’ Slack/Email

#### ç¶™ç¶šçš„ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ
1. **ãƒãƒ¼ã‚¸æ¤œçŸ¥** â†’ main/develop branch
2. **ãƒ“ãƒ«ãƒ‰å®Ÿè¡Œ** â†’ Docker build
3. **çµ±åˆãƒ†ã‚¹ãƒˆ** â†’ E2E tests
4. **ãƒ‡ãƒ—ãƒ­ã‚¤å®Ÿè¡Œ** â†’ Staging/Production
5. **ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯** â†’ API monitoring

ã“ã®åŒ…æ‹¬çš„ãªãƒ†ã‚¹ãƒˆæˆ¦ç•¥ã«ã‚ˆã‚Šã€Phase 2ã®å®Ÿè£…å“è³ªã‚’ç¢ºä¿ã—ã€å®‰å®šã—ãŸMVPã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚