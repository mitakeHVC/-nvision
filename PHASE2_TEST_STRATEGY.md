# Phase 2 „ÉÜ„Çπ„ÉàÊà¶Áï•„Å®GitÊà¶Áï•

## üß™ „ÉÜ„Çπ„ÉàÊà¶Áï•„ÅÆË©≥Á¥∞Âåñ

### „ÉÜ„Çπ„ÉàÁ®ÆÈ°û„Å®ÂÆüË°å„Çø„Ç§„Éü„É≥„Ç∞

| „ÉÜ„Çπ„ÉàÁ®ÆÈ°û | ÂÆüË°å„Çø„Ç§„Éü„É≥„Ç∞ | „Ç´„Éê„É¨„ÉÉ„Ç∏ÁõÆÊ®ô | ÂÆüË°åÊôÇÈñìÁõÆÊ®ô | Ë≤¨‰ªªËÄÖ |
|------------|----------------|----------------|--------------|--------|
| Âçò‰Ωì„ÉÜ„Çπ„Éà | ÂêÑ„Ç≥„Éü„ÉÉ„ÉàÊôÇ | 90%‰ª•‰∏ä | 30Áßí‰ª•ÂÜÖ | ÈñãÁô∫ËÄÖ |
| Áµ±Âêà„ÉÜ„Çπ„Éà | PR‰ΩúÊàêÊôÇ | 80%‰ª•‰∏ä | 2ÂàÜ‰ª•ÂÜÖ | CI/CD |
| E2E„ÉÜ„Çπ„Éà | „Éû„Éº„Ç∏Ââç | ‰∏ªË¶Å„Éï„É≠„Éº100% | 5ÂàÜ‰ª•ÂÜÖ | CI/CD |
| „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÉÜ„Çπ„Éà | „É™„É™„Éº„ÇπÂâç | ÂÖ®API | 10ÂàÜ‰ª•ÂÜÖ | QA |
| „Çª„Ç≠„É•„É™„ÉÜ„Ç£„ÉÜ„Çπ„Éà | „É™„É™„Éº„ÇπÂâç | ÂÖ®„Ç®„É≥„Éâ„Éù„Ç§„É≥„Éà | 15ÂàÜ‰ª•ÂÜÖ | QA |

### „ÉÜ„Çπ„Éà„Ç´„Éê„É¨„ÉÉ„Ç∏ÁõÆÊ®ô

#### „Ç≥„É≥„Éù„Éº„Éç„É≥„ÉàÂà•„Ç´„Éê„É¨„ÉÉ„Ç∏
- **„Éá„Éº„Çø„É¢„Éá„É´**: 95%‰ª•‰∏ä
- **„É™„Éù„Ç∏„Éà„É™Â±§**: 90%‰ª•‰∏ä
- **„Çµ„Éº„Éì„ÇπÂ±§**: 90%‰ª•‰∏ä
- **APIÂ±§**: 85%‰ª•‰∏ä
- **„Éü„Éâ„É´„Ç¶„Çß„Ç¢**: 80%‰ª•‰∏ä
- **Áµ±Âêà„ÉÜ„Çπ„Éà**: 80%‰ª•‰∏ä

#### Ê©üËÉΩÂà•„Ç´„Éê„É¨„ÉÉ„Ç∏
- **CRUDÊìç‰Ωú**: 100%
- **Ë™çË®º„ÉªË™çÂèØ**: 100%
- **Ê§úÁ¥¢Ê©üËÉΩ**: 95%‰ª•‰∏ä
- **„Ç®„É©„Éº„Éè„É≥„Éâ„É™„É≥„Ç∞**: 90%‰ª•‰∏ä
- **„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ**: ‰∏ªË¶Å„Ç®„É≥„Éâ„Éù„Ç§„É≥„Éà100%

## üìã „ÉÜ„Çπ„ÉàÂÆüË£ÖË®àÁîª

### Step 1: CI/CDÂü∫Áõ§ÊßãÁØâÊôÇ„ÅÆ„ÉÜ„Çπ„Éà

#### 1.1 GitHub ActionsË®≠ÂÆö„ÉÜ„Çπ„Éà
**„Éï„Ç°„Ç§„É´**: [`/.github/workflows/test-ci.yml`](.github/workflows/test-ci.yml)
```yaml
name: Test CI Pipeline
on:
  workflow_dispatch:  # Manual trigger for testing

jobs:
  test-pipeline:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Test pipeline setup
      run: echo "CI pipeline test successful"
    
    - name: Validate workflow syntax
      run: |
        # Validate YAML syntax
        python -c "import yaml; yaml.safe_load(open('.github/workflows/ci.yml'))"
```

#### 1.2 ÂìÅË≥™„Ç≤„Éº„Éà„ÉÜ„Çπ„Éà
**„Éï„Ç°„Ç§„É´**: [`/tests/quality/test_code_quality.py`](tests/quality/test_code_quality.py)
```python
"""Code quality tests."""
import subprocess
import pytest

def test_black_formatting():
    """Test that code is properly formatted with black."""
    result = subprocess.run(
        ["black", "--check", "src", "tests"],
        capture_output=True,
        text=True
    )
    assert result.returncode == 0, f"Code formatting issues: {result.stdout}"

def test_flake8_linting():
    """Test that code passes flake8 linting."""
    result = subprocess.run(
        ["flake8", "src", "tests"],
        capture_output=True,
        text=True
    )
    assert result.returncode == 0, f"Linting issues: {result.stdout}"

def test_mypy_type_checking():
    """Test that code passes mypy type checking."""
    result = subprocess.run(
        ["mypy", "src"],
        capture_output=True,
        text=True
    )
    assert result.returncode == 0, f"Type checking issues: {result.stdout}"
```

### Step 2: FastAPIÂü∫Áõ§ÂÆüË£ÖÊôÇ„ÅÆ„ÉÜ„Çπ„Éà

#### 2.1 „Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥Ëµ∑Âãï„ÉÜ„Çπ„Éà
**„Éï„Ç°„Ç§„É´**: [`/tests/api/test_application.py`](tests/api/test_application.py)
```python
"""Application startup and configuration tests."""
import pytest
from fastapi.testclient import TestClient
from src.api.main import app

client = TestClient(app)

def test_application_startup():
    """Test that application starts successfully."""
    response = client.get("/")
    assert response.status_code == 200
    assert "nvision API is running" in response.json()["message"]

def test_openapi_schema_generation():
    """Test that OpenAPI schema is generated correctly."""
    response = client.get("/openapi.json")
    assert response.status_code == 200
    schema = response.json()
    assert "openapi" in schema
    assert schema["info"]["title"] == "nvision API"

def test_docs_endpoints():
    """Test that documentation endpoints are accessible."""
    # Swagger UI
    response = client.get("/docs")
    assert response.status_code == 200
    
    # ReDoc
    response = client.get("/redoc")
    assert response.status_code == 200

def test_cors_middleware():
    """Test CORS middleware configuration."""
    response = client.options("/", headers={
        "Origin": "http://localhost:3000",
        "Access-Control-Request-Method": "GET"
    })
    assert response.status_code == 200
    assert "access-control-allow-origin" in response.headers
```

#### 2.2 „Éü„Éâ„É´„Ç¶„Çß„Ç¢„ÉÜ„Çπ„Éà
**„Éï„Ç°„Ç§„É´**: [`/tests/api/test_middleware.py`](tests/api/test_middleware.py)
```python
"""Middleware tests."""
import pytest
import time
from unittest.mock import patch
from fastapi.testclient import TestClient
from src.api.main import app

client = TestClient(app)

def test_logging_middleware():
    """Test logging middleware functionality."""
    with patch('src.api.middleware.logger') as mock_logger:
        response = client.get("/api/v1/health")
        assert response.status_code == 200
        
        # Verify logging calls
        assert mock_logger.info.call_count >= 2  # Request and response logs
        assert "X-Process-Time" in response.headers

def test_error_handling_middleware():
    """Test error handling middleware."""
    # This would test a route that intentionally raises an exception
    # For now, we'll test that the middleware is properly configured
    assert hasattr(app, 'middleware_stack')

def test_process_time_header():
    """Test that process time header is added."""
    response = client.get("/api/v1/health")
    assert "X-Process-Time" in response.headers
    process_time = float(response.headers["X-Process-Time"])
    assert process_time >= 0
```

#### 2.3 Ë®≠ÂÆö„ÉÜ„Çπ„Éà
**„Éï„Ç°„Ç§„É´**: [`/tests/api/test_config.py`](tests/api/test_config.py)
```python
"""Configuration tests."""
import pytest
from src.api.config import Settings, get_settings

def test_settings_creation():
    """Test settings creation with default values."""
    settings = Settings()
    assert settings.PROJECT_NAME == "nvision"
    assert settings.API_V1_STR == "/api/v1"
    assert settings.ALGORITHM == "HS256"

def test_settings_caching():
    """Test that settings are cached."""
    settings1 = get_settings()
    settings2 = get_settings()
    assert settings1 is settings2  # Same instance due to lru_cache

def test_environment_variable_override():
    """Test that environment variables override defaults."""
    import os
    original_value = os.environ.get("PROJECT_NAME")
    
    try:
        os.environ["PROJECT_NAME"] = "test-nvision"
        # Clear the cache
        get_settings.cache_clear()
        settings = get_settings()
        assert settings.PROJECT_NAME == "test-nvision"
    finally:
        if original_value:
            os.environ["PROJECT_NAME"] = original_value
        else:
            os.environ.pop("PROJECT_NAME", None)
        get_settings.cache_clear()
```

### Step 3: „Éá„Éº„Çø„Ç¢„ÇØ„Çª„ÇπÂ±§ÂÆüË£ÖÊôÇ„ÅÆ„ÉÜ„Çπ„Éà

#### 3.1 „Éá„Éº„Çø„Éô„Éº„ÇπÊé•Á∂ö„ÉÜ„Çπ„Éà
**„Éï„Ç°„Ç§„É´**: [`/tests/database/test_neo4j_client.py`](tests/database/test_neo4j_client.py)
```python
"""Neo4j client tests."""
import pytest
from unittest.mock import Mock, patch, MagicMock
from src.database.neo4j_client import Neo4jClient

@pytest.fixture
def mock_driver():
    """Mock Neo4j driver."""
    driver = Mock()
    session = Mock()
    driver.session.return_value.__enter__.return_value = session
    driver.session.return_value.__exit__.return_value = None
    return driver, session

def test_neo4j_client_initialization():
    """Test Neo4j client initialization."""
    with patch('src.database.neo4j_client.GraphDatabase.driver') as mock_driver:
        client = Neo4jClient("bolt://localhost:7687", "neo4j", "password")
        mock_driver.assert_called_once_with(
            "bolt://localhost:7687", 
            auth=("neo4j", "password")
        )

def test_verify_connectivity(mock_driver):
    """Test connectivity verification."""
    driver, session = mock_driver
    
    # Mock successful connectivity test
    result = Mock()
    result.single.return_value = {"test": 1}
    session.run.return_value = result
    
    with patch('src.database.neo4j_client.GraphDatabase.driver', return_value=driver):
        client = Neo4jClient("bolt://localhost:7687", "neo4j", "password")
        assert client.verify_connectivity() == True

def test_execute_query(mock_driver):
    """Test query execution."""
    driver, session = mock_driver
    
    # Mock query result
    record1 = Mock()
    record1.data.return_value = {"name": "John", "age": 30}
    record2 = Mock()
    record2.data.return_value = {"name": "Jane", "age": 25}
    
    result = Mock()
    result.__iter__.return_value = [record1, record2]
    session.run.return_value = result
    
    with patch('src.database.neo4j_client.GraphDatabase.driver', return_value=driver):
        client = Neo4jClient("bolt://localhost:7687", "neo4j", "password")
        results = client.execute_query("MATCH (n) RETURN n")
        
        assert len(results) == 2
        assert results[0]["name"] == "John"
        assert results[1]["name"] == "Jane"

def test_execute_write_query(mock_driver):
    """Test write query execution."""
    driver, session = mock_driver
    
    # Mock transaction
    tx = Mock()
    tx.run.return_value = []
    session.write_transaction.return_value = []
    
    with patch('src.database.neo4j_client.GraphDatabase.driver', return_value=driver):
        client = Neo4jClient("bolt://localhost:7687", "neo4j", "password")
        result = client.execute_write_query("CREATE (n:Person {name: $name})", {"name": "John"})
        
        session.write_transaction.assert_called_once()

def test_client_close(mock_driver):
    """Test client connection closing."""
    driver, session = mock_driver
    
    with patch('src.database.neo4j_client.GraphDatabase.driver', return_value=driver):
        client = Neo4jClient("bolt://localhost:7687", "neo4j", "password")
        client.close()
        driver.close.assert_called_once()
```

#### 3.2 „É™„Éù„Ç∏„Éà„É™„ÉÜ„Çπ„Éà
**„Éï„Ç°„Ç§„É´**: [`/tests/repositories/test_base_repository.py`](tests/repositories/test_base_repository.py)
```python
"""Base repository tests."""
import pytest
from abc import ABC
from unittest.mock import Mock
from src.repositories.base_repository import BaseRepository

class TestRepository(BaseRepository):
    """Test implementation of base repository."""
    
    def create(self, entity):
        return entity
    
    def get_by_id(self, entity_id):
        return {"id": entity_id}
    
    def update(self, entity_id, updates):
        return {"id": entity_id, **updates}
    
    def delete(self, entity_id):
        return True
    
    def list(self, limit=100, offset=0):
        return [{"id": i} for i in range(offset, offset + min(limit, 10))]

def test_base_repository_abstract():
    """Test that BaseRepository is abstract."""
    with pytest.raises(TypeError):
        BaseRepository(Mock())

def test_repository_implementation():
    """Test repository implementation."""
    mock_client = Mock()
    repo = TestRepository(mock_client)
    
    # Test create
    entity = {"name": "test"}
    result = repo.create(entity)
    assert result == entity
    
    # Test get_by_id
    result = repo.get_by_id("123")
    assert result["id"] == "123"
    
    # Test update
    result = repo.update("123", {"name": "updated"})
    assert result["id"] == "123"
    assert result["name"] == "updated"
    
    # Test delete
    result = repo.delete("123")
    assert result == True
    
    # Test list
    result = repo.list(5, 0)
    assert len(result) == 5
    assert result[0]["id"] == 0
```

#### 3.3 Áµ±Âêà„ÉÜ„Çπ„Éà
**„Éï„Ç°„Ç§„É´**: [`/tests/integration/test_data_access_integration.py`](tests/integration/test_data_access_integration.py)
```python
"""Data access integration tests."""
import pytest
from unittest.mock import Mock, patch
from src.services.data_service import DataService
from src.database.connection_manager import ConnectionManager
from src.data_models.crm_models import Customer

@pytest.fixture
def mock_connection_manager():
    """Mock connection manager."""
    manager = Mock(spec=ConnectionManager)
    manager.neo4j = Mock()
    manager.chroma = Mock()
    return manager

@pytest.fixture
def data_service(mock_connection_manager):
    """Data service fixture."""
    return DataService(mock_connection_manager)

@pytest.fixture
def sample_customer():
    """Sample customer fixture."""
    return Customer(
        customer_id="test-123",
        first_name="Test",
        last_name="User",
        email="test@example.com",
        phone="123-456-7890"
    )

def test_data_service_customer_operations(data_service, sample_customer, mock_connection_manager):
    """Test data service customer operations."""
    # Mock repository responses
    with patch('src.services.data_service.CustomerRepository') as MockCustomerRepo:
        mock_repo = MockCustomerRepo.return_value
        mock_repo.create.return_value = sample_customer
        mock_repo.get_by_id.return_value = sample_customer
        mock_repo.list.return_value = [sample_customer]
        
        # Test create
        result = data_service.create_customer(sample_customer)
        assert result.customer_id == "test-123"
        mock_repo.create.assert_called_once_with(sample_customer)
        
        # Test get
        result = data_service.get_customer("test-123")
        assert result.customer_id == "test-123"
        mock_repo.get_by_id.assert_called_once_with("test-123")
        
        # Test list
        results = data_service.list_customers()
        assert len(results) == 1
        assert results[0].customer_id == "test-123"
        mock_repo.list.assert_called_once()

def test_data_service_error_handling(data_service, mock_connection_manager):
    """Test data service error handling."""
    with patch('src.services.data_service.CustomerRepository') as MockCustomerRepo:
        mock_repo = MockCustomerRepo.return_value
        mock_repo.get_by_id.side_effect = Exception("Database error")
        
        with pytest.raises(Exception) as exc_info:
            data_service.get_customer("test-123")
        
        assert "Database error" in str(exc_info.value)
```

## üîÑ GitÊà¶Áï•„ÅÆË©≥Á¥∞Âåñ

### „Éñ„É©„É≥„ÉÅÊà¶Áï•

#### „Éñ„É©„É≥„ÉÅÂëΩÂêçË¶èÂâá
```
main                    # Êú¨Áï™Áí∞Â¢É„Éñ„É©„É≥„ÉÅ
develop                 # ÈñãÁô∫Áµ±Âêà„Éñ„É©„É≥„ÉÅ
feature/[step-name]     # Ê©üËÉΩÈñãÁô∫„Éñ„É©„É≥„ÉÅ
hotfix/[issue-name]     # Á∑äÊÄ•‰øÆÊ≠£„Éñ„É©„É≥„ÉÅ
release/[version]       # „É™„É™„Éº„Çπ„Éñ„É©„É≥„ÉÅ
```

#### „Éñ„É©„É≥„ÉÅÈÅãÁî®„Éï„É≠„Éº
```mermaid
gitGraph
    commit id: "Initial"
    branch develop
    checkout develop
    commit id: "Setup"
    
    branch feature/ci-cd-setup
    checkout feature/ci-cd-setup
    commit id: "CI/CD"
    commit id: "Tests"
    
    checkout develop
    merge feature/ci-cd-setup
    
    branch feature/fastapi-foundation
    checkout feature/fastapi-foundation
    commit id: "FastAPI"
    commit id: "Middleware"
    
    checkout develop
    merge feature/fastapi-foundation
    
    branch release/v0.1.0
    checkout release/v0.1.0
    commit id: "Release prep"
    
    checkout main
    merge release/v0.1.0
    tag: "v0.1.0"
    
    checkout develop
    merge release/v0.1.0
```

### „Ç≥„Éü„ÉÉ„Éà„É°„ÉÉ„Çª„Éº„Ç∏Ë¶èÂâá

#### Conventional CommitsÂΩ¢Âºè
```
<type>[optional scope]: <description>

[optional body]

[optional footer(s)]
```

#### „Çø„Ç§„ÉóÂÆöÁæ©
- **feat**: Êñ∞Ê©üËÉΩËøΩÂä†
- **fix**: „Éê„Ç∞‰øÆÊ≠£
- **docs**: „Éâ„Ç≠„É•„É°„É≥„ÉàÊõ¥Êñ∞
- **style**: „Ç≥„Éº„Éâ„Çπ„Çø„Ç§„É´Â§âÊõ¥ÔºàÊ©üËÉΩ„Å´ÂΩ±Èüø„Å™„ÅóÔºâ
- **refactor**: „É™„Éï„Ç°„ÇØ„Çø„É™„É≥„Ç∞
- **perf**: „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊîπÂñÑ
- **test**: „ÉÜ„Çπ„ÉàËøΩÂä†„Éª‰øÆÊ≠£
- **chore**: „Éì„É´„Éâ„Éó„É≠„Çª„Çπ„ÉªË£úÂä©„ÉÑ„Éº„É´Â§âÊõ¥

#### „Ç≥„Éü„ÉÉ„Éà„É°„ÉÉ„Çª„Éº„Ç∏‰æã
```bash
feat(api): implement FastAPI foundation with basic middleware

- Add FastAPI application setup
- Implement CORS and logging middleware
- Add health check endpoints
- Configure OpenAPI documentation

Closes #123
```

### „Éû„Éº„Ç∏Êà¶Áï•

#### Pull RequestË¶Å‰ª∂
1. **ÂøÖÈ†à„ÉÅ„Çß„ÉÉ„ÇØ**:
   - [ ] CI/CD„Éë„Ç§„Éó„É©„Ç§„É≥ÊàêÂäü
   - [ ] „Ç≥„Éº„Éâ„É¨„Éì„É•„ÉºÊâøË™çÔºàÊúÄ‰Ωé1ÂêçÔºâ
   - [ ] „ÉÜ„Çπ„Éà„Ç´„Éê„É¨„ÉÉ„Ç∏Âü∫Ê∫ñÈÅîÊàê
   - [ ] „Ç≥„Éº„ÉâÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÈÄöÈÅé

2. **„É¨„Éì„É•„ÉºË¶≥ÁÇπ**:
   - „Ç≥„Éº„ÉâÂìÅË≥™„Å®ÂèØË™≠ÊÄß
   - „ÉÜ„Çπ„Éà„ÅÆÂ¶•ÂΩìÊÄß
   - „Çª„Ç≠„É•„É™„ÉÜ„Ç£ËÄÉÊÖÆ‰∫ãÈ†Ö
   - „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÂΩ±Èüø
   - „Éâ„Ç≠„É•„É°„É≥„ÉàÊõ¥Êñ∞

#### „Éû„Éº„Ç∏ÊñπÊ≥ï
- **Squash and merge**: Ê©üËÉΩ„Éñ„É©„É≥„ÉÅ ‚Üí develop
- **Merge commit**: develop ‚Üí main
- **Fast-forward**: hotfix ‚Üí main

### Git HooksË®≠ÂÆö

#### Pre-commit Hook
**„Éï„Ç°„Ç§„É´**: [`/.git/hooks/pre-commit`](.git/hooks/pre-commit)
```bash
#!/bin/sh
# Pre-commit hook for code quality checks

echo "Running pre-commit checks..."

# Run black formatting check
echo "Checking code formatting..."
black --check src tests
if [ $? -ne 0 ]; then
    echo "‚ùå Code formatting check failed. Run 'black src tests' to fix."
    exit 1
fi

# Run flake8 linting
echo "Running linting..."
flake8 src tests
if [ $? -ne 0 ]; then
    echo "‚ùå Linting check failed. Fix the issues above."
    exit 1
fi

# Run type checking
echo "Running type checking..."
mypy src
if [ $? -ne 0 ]; then
    echo "‚ùå Type checking failed. Fix the issues above."
    exit 1
fi

# Run tests
echo "Running tests..."
pytest tests/ -x
if [ $? -ne 0 ]; then
    echo "‚ùå Tests failed. Fix the failing tests."
    exit 1
fi

echo "‚úÖ All pre-commit checks passed!"
```

#### Commit-msg Hook
**„Éï„Ç°„Ç§„É´**: [`/.git/hooks/commit-msg`](.git/hooks/commit-msg)
```bash
#!/bin/sh
# Commit message validation hook

commit_regex='^(feat|fix|docs|style|refactor|perf|test|chore)(\(.+\))?: .{1,50}'

if ! grep -qE "$commit_regex" "$1"; then
    echo "‚ùå Invalid commit message format!"
    echo "Format: <type>[optional scope]: <description>"
    echo "Example: feat(api): add user authentication"
    exit 1
fi

echo "‚úÖ Commit message format is valid!"
```

## üìä „ÉÜ„Çπ„ÉàÂÆüË°å„Å®„É¨„Éù„Éº„Éà

### „ÉÜ„Çπ„ÉàÂÆüË°å„Ç≥„Éû„É≥„Éâ

#### ÈñãÁô∫ÊôÇ„ÉÜ„Çπ„Éà
```bash
# Âçò‰Ωì„ÉÜ„Çπ„ÉàÂÆüË°å
pytest tests/unit/ -v

# Áµ±Âêà„ÉÜ„Çπ„ÉàÂÆüË°å
pytest tests/integration/ -v

# „Ç´„Éê„É¨„ÉÉ„Ç∏‰ªò„Åç„ÉÜ„Çπ„ÉàÂÆüË°å
pytest --cov=src --cov-report=html --cov-report=term

# ÁâπÂÆö„É¢„Ç∏„É•„Éº„É´„ÅÆ„ÉÜ„Çπ„Éà
pytest tests/api/ -v

# Â§±ÊïóÊôÇÂÅúÊ≠¢
pytest -x

# ‰∏¶ÂàóÂÆüË°å
pytest -n auto
```

#### CI/CD„ÉÜ„Çπ„Éà
```bash
# ÂÖ®„ÉÜ„Çπ„ÉàÂÆüË°åÔºàCIÁí∞Â¢ÉÔºâ
pytest --cov=src --cov-report=xml --cov-fail-under=85

# „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÉÜ„Çπ„Éà
pytest tests/performance/ --benchmark-only

# „Çª„Ç≠„É•„É™„ÉÜ„Ç£„ÉÜ„Çπ„Éà
bandit -r src/
safety check
```

### „ÉÜ„Çπ„Éà„É¨„Éù„Éº„ÉàÁîüÊàê

#### „Ç´„Éê„É¨„ÉÉ„Ç∏„É¨„Éù„Éº„Éà
```bash
# HTMLÂΩ¢Âºè„ÅÆ„Ç´„Éê„É¨„ÉÉ„Ç∏„É¨„Éù„Éº„ÉàÁîüÊàê
pytest --cov=src --cov-report=html
open htmlcov/index.html

# XMLÂΩ¢ÂºèÔºàCIÁî®Ôºâ
pytest --cov=src --cov-report=xml
```

#### „ÉÜ„Çπ„ÉàÁµêÊûú„É¨„Éù„Éº„Éà
```bash
# JUnitÂΩ¢Âºè„ÅÆ„ÉÜ„Çπ„ÉàÁµêÊûú
pytest --junitxml=test-results.xml

# HTMLÂΩ¢Âºè„ÅÆ„ÉÜ„Çπ„ÉàÁµêÊûú
pytest --html=test-report.html --self-contained-html
```

## üö® ÂìÅË≥™„Ç≤„Éº„Éà„Å®Ëá™ÂãïÂåñ

### ÂìÅË≥™Âü∫Ê∫ñ

| ÊåáÊ®ô | ÊúÄÂ∞èÂÄ§ | ÁõÆÊ®ôÂÄ§ | Ê∏¨ÂÆöÊñπÊ≥ï |
|------|--------|--------|----------|
| „ÉÜ„Çπ„Éà„Ç´„Éê„É¨„ÉÉ„Ç∏ | 85% | 90% | pytest-cov |
| „Ç≥„Éº„ÉâÂìÅË≥™„Çπ„Ç≥„Ç¢ | B | A | SonarQube |
| „Çª„Ç≠„É•„É™„ÉÜ„Ç£„Çπ„Ç≥„Ç¢ | 8/10 | 10/10 | Bandit + Safety |
| „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ | 500ms | 200ms | Locust |
| ÂèØÁî®ÊÄß | 99% | 99.9% | Uptime monitoring |

### Ëá™ÂãïÂåñ„ÉØ„Éº„ÇØ„Éï„É≠„Éº

#### Á∂ôÁ∂öÁöÑ„Ç§„É≥„ÉÜ„Ç∞„É¨„Éº„Ç∑„Éß„É≥
1. **„Ç≥„Éº„ÉâÂ§âÊõ¥Ê§úÁü•** ‚Üí GitHub webhook
2. **ÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÂÆüË°å** ‚Üí GitHub Actions
3. **„ÉÜ„Çπ„ÉàÂÆüË°å** ‚Üí pytest + coverage
4. **„Çª„Ç≠„É•„É™„ÉÜ„Ç£„Çπ„Ç≠„É£„É≥** ‚Üí Bandit + Safety
5. **ÁµêÊûúÈÄöÁü•** ‚Üí Slack/Email

#### Á∂ôÁ∂öÁöÑ„Éá„Éó„É≠„Ç§„É°„É≥„Éà
1. **„Éû„Éº„Ç∏Ê§úÁü•** ‚Üí main/develop branch
2. **„Éì„É´„ÉâÂÆüË°å** ‚Üí Docker build
3. **Áµ±Âêà„ÉÜ„Çπ„Éà** ‚Üí E2E tests
4. **„Éá„Éó„É≠„Ç§ÂÆüË°å** ‚Üí Staging/Production
5. **„Éò„É´„Çπ„ÉÅ„Çß„ÉÉ„ÇØ** ‚Üí API monitoring

„Åì„ÅÆÂåÖÊã¨ÁöÑ„Å™„ÉÜ„Çπ„ÉàÊà¶Áï•„Å´„Çà„Çä„ÄÅPhase 2„ÅÆÂÆüË£ÖÂìÅË≥™„ÇíÁ¢∫‰øù„Åó„ÄÅÂÆâÂÆö„Åó„ÅüMVP„ÇíÊßãÁØâ„Åß„Åç„Åæ„Åô„ÄÇ